### DPO(Direct Preference Optimization)

#### 原理 
1. 


#### 数据
1. 格式:
- 举例: {"question":'...',"accept":"...","reject":"..."}
```python


```
```
```
